import os
import json
import argparse
from pathlib import Path
from typing import List, Dict
from dotenv import load_dotenv
from mistralai import Mistral

from .types import CodeProblem
from .verify import load_problems_from_file, evaluate_all_problems

load_dotenv()

def get_mistral_client() -> Mistral:
    """Initialize and return Mistral client."""
    api_key = os.getenv("MISTRAL_API_KEY")
    if not api_key:
        raise ValueError("MISTRAL_API_KEY environment variable is required")
    return Mistral(api_key=api_key)

def create_code_generation_prompt(problem: CodeProblem) -> str:
    """Create prompt for LLM to generate code for the sequence."""
    
    sequence_str = str(problem.sequence_short)
    
    prompt = f"""You need to analyze the pattern in this sequence and write Python code to generate it:

Given sequence (first 10 elements): {sequence_str}

Instructions:
1. First, analyze the pattern in the sequence to find the mathematical formula
2. Write Python code that generates the sequence using your discovered formula
3. Your function should generate exactly 20 elements of the sequence (not just the 10 shown)
4. Store the final result in a variable called 'result'
5. Put your Python code between <python> and </python> tags
6. Do NOT hardcode the values - you must use a mathematical formula
7. Do not reason too much to find the formula.

Hint: The sequence is generated by a mathematical formula using modular linear/quadratic arithmetic and polynomial modular arithmetic.


IMPORTANT: Generate exactly 20 elements, not 10. The pattern should extend beyond what's shown.

Example format:
<python>
def generate_sequence():
    # Your mathematical formula here
    sequence = []
    for i in range(1, 21):  # Generate 20 elements
        # Apply your formula
        value = ...  # Your formula here
        sequence.append(value)
    return sequence

result = generate_sequence()
</python>

Now analyze the pattern and write the code:"""

    return prompt

def get_llm_response(problem: CodeProblem, client: Mistral, max_retries: int = 3) -> str:
    """Get LLM response for code generation problem."""
    prompt = create_code_generation_prompt(problem)
    
    for attempt in range(max_retries):
        try:
            response = client.chat.complete(
                model="mistral-medium-latest",
                messages=[
                    {
                        "role": "user",
                        "content": prompt
                    }
                ],
                max_tokens=3000,
            )
            
            if response.choices and response.choices[0].message:
                answer = response.choices[0].message.content.strip()
                print(answer)
                return answer
                
        except Exception as e:
            print(f"Attempt {attempt + 1} failed: {e}")
            if attempt == max_retries - 1:
                raise
    
    return ""

def run_evaluation(problems_file: str, verbose: bool = False) -> Dict:
    """Run evaluation on all problems from a file."""
    problems = load_problems_from_file(problems_file)
    client = get_mistral_client()
    
    print(f"Evaluating {len(problems)} code generation problems...")
    
    llm_responses = []
    for i, problem in enumerate(problems):
        print(f"Getting LLM response for problem {i + 1}/{len(problems)}...")
        
        try:
            llm_response = get_llm_response(problem, client)
            llm_responses.append(llm_response)
        except Exception as e:
            print(f"Error getting response for problem {i + 1}: {e}")
            llm_responses.append("")
    
    results = evaluate_all_problems(problems, llm_responses)
    
    if verbose:
        for i, (problem, result) in enumerate(zip(problems, results["detailed_results"])):
            print(f"\nProblem {i + 1}: {problem.description}")
            print(f"Given (short): {problem.sequence_short}")
            print(f"Expected (long): {problem.sequence_long}")
            print(f"Success: {result['success']}")
            if result["details"] and result["details"]["generated_sequence"]:
                print(f"Generated: {result['details']['generated_sequence']}")
            if result["details"] and result["details"]["execution_error"]:
                print(f"Error: {result['details']['execution_error']}")
            if result["details"] and result["details"]["extracted_code"]:
                print(f"Code:\n{result['details']['extracted_code']}")
            print("-" * 50)
    
    return results

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Evaluate LLM performance on code generation")
    parser.add_argument(
        "--problems-file",
        type=str,
        default="reasoning/data/problems.json",
        help="Path to JSON file containing code generation problems"
    )
    parser.add_argument(
        "--verbose",
        action="store_true",
        help="Print detailed results for each problem"
    )
    parser.add_argument(
        "--output",
        type=str,
        help="Save detailed results to JSON file"
    )
    
    args = parser.parse_args()
    
    if not Path(args.problems_file).exists():
        print(f"Error: Problems file {args.problems_file} does not exist")
        print("Generate problems first using: uv run python -m reasoning.generate --output reasoning/data/problems.json")
        exit(1)
    
    results = run_evaluation(args.problems_file, args.verbose)
    
    print("\n" + "=" * 60)
    print("EVALUATION SUMMARY")
    print("=" * 60)
    print(f"Total problems: {results['total_problems']}")
    print(f"Successful solutions: {results['successful']}")
    print(f"Success rate: {results['success_rate']:.2%}")
    print(f"Code extraction failures: {results['code_extraction_failures']}")
    print(f"Execution errors: {results['execution_errors']}")
    print(f"API errors: {results['api_errors']}")
    
    if args.output:
        with open(args.output, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        print(f"\nDetailed results saved to {args.output}") 